{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reprocessing procedure for Las Campanas Observatory Swope data using BANZAI-Imaging\n",
    "## Matt Daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up relevant runtime configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "from banzai.calibrations import make_master_calibrations\n",
    "from banzai import settings\n",
    "from banzai import dbs\n",
    "from banzai.utils.stage_utils import run_pipeline_stages\n",
    "from banzai.logs import set_log_level\n",
    "import banzai.main\n",
    "\n",
    "import pkg_resources\n",
    "import requests\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We construct multiple runtime contexts, one for each night-obs. \n",
    "\n",
    "This is so that data taken on one night is reduced with calibrations from that same night. We don't want to mix them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_log_level('DEBUG')\n",
    "logger = logging.getLogger('banzai')\n",
    "\n",
    "# Define the data directories on Apophis\n",
    "# For each raw data dir, define an MEF-combined data dir\n",
    "raw_data_dirs = ['/apophis/eng/rocks/Swope/ut220925/fits_raw/', '/apophis/eng/rocks/Swope/ut221002/fits_raw/']\n",
    "mef_combined_data_dirs = [os.path.join(os.getcwd(), 'swope_data', 'mef_combined', '220925'),\n",
    "                          os.path.join(os.getcwd(), 'swope_data', 'mef_combined', '221002')]\n",
    "\n",
    "# Create the MEF data dirs if they don't exist\n",
    "for path in mef_combined_data_dirs:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "# Now construct our context objects. We want to use a separate DB for each data dir.\n",
    "contexts = []\n",
    "\n",
    "for combined_dir in mef_combined_data_dirs:\n",
    "    os.environ['OPENTSDB_PYTHON_METRICS_TEST_MODE'] = 'True'\n",
    "\n",
    "    settings.processed_path = os.path.join(os.getcwd(), 'test_data')\n",
    "    settings.fpack=True\n",
    "    settings.db_address = f\"sqlite:///{os.path.join(combined_dir, 'test.db')}\"\n",
    "    settings.no_bpm = True\n",
    "    settings.reduction_level = 91\n",
    "\n",
    "    # set up the context object.\n",
    "    context = banzai.main.parse_args(settings, parse_system_args=False)\n",
    "    contexts.append(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the data from each directory and combine it into MEFs\n",
    "This branch of BANZAI expects that each amplifier have an extension in an MEF. BANZAI can then do the work of stitching them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_dir, combined_dir in zip(raw_data_dirs, mef_combined_data_dirs):\n",
    "    print(f\"Combining files in {raw_dir}\")\n",
    "    \n",
    "    prefix_set = set()\n",
    "    for file_path in glob(os.path.join(raw_dir, '*.fits.fz')):\n",
    "        # extract the unique prefixes from the raw data directory\n",
    "        prefix_set.add(re.sub('c\\d', '', file_path))\n",
    "\n",
    "    # now create MEFs from the groupings!\n",
    "    for prefix in prefix_set:\n",
    "        filepaths = [prefix.replace('.fits.fz', f'c{i}.fits.fz') for i in [4,2,3,1]]\n",
    "\n",
    "        image_hdus = fits.HDUList([fits.PrimaryHDU()])\n",
    "        for path in filepaths:\n",
    "            image_hdus.append(fits.open(path)['COMPRESSED_IMAGE'])\n",
    "\n",
    "        if image_hdus[1].header['EXPTYPE'].upper() == 'FOCUS':\n",
    "            continue\n",
    "\n",
    "        # Now do some munging of the headers.\n",
    "        # 1 is top right, rotate 90 counterclockwise and invert x (x -> -y, y-> -x)\n",
    "        image_hdus[1].data = image_hdus[1].data.T[::-1, ::-1]\n",
    "        image_hdus[1].header['DATASEC'] = '[129:2184,129:2176]'\n",
    "        image_hdus[1].header['BIASSEC'] = '[1:128,1:2176]'\n",
    "        image_hdus[1].header['DETSEC'] = '[2057:4112,2049:4096]'\n",
    "        image_hdus[1].header.pop('TRIMSEC')\n",
    "\n",
    "        # 2 is bottom left rotate 90 clockwise and invert x (y -> x x -> y)\n",
    "        image_hdus[2].data = image_hdus[2].data.T\n",
    "        image_hdus[2].header['DATASEC'] = '[1:2056,1:2048]'\n",
    "        image_hdus[2].header['BIASSEC'] ='[2057:2084,1:2176]'\n",
    "        image_hdus[2].header['DETSEC'] = '[1:2056,1:2048]'\n",
    "        image_hdus[2].header.pop('TRIMSEC')\n",
    "\n",
    "        # 3 bottom right rotate 90 counterclockwise (x-> -y, y - > x)\n",
    "        image_hdus[3].data = image_hdus[3].data.T[:, ::-1]\n",
    "        image_hdus[3].header['DATASEC'] = '[129:2184,1:2048]'\n",
    "        image_hdus[3].header['BIASSEC'] ='[1:129,1:2176]'\n",
    "        image_hdus[3].header['DETSEC'] = '[2057:4112,1:2048]'\n",
    "        image_hdus[3].header.pop('TRIMSEC')\n",
    "\n",
    "        # 4 is top left 270 deg  counterclockwise (x -> y, y->-x)\n",
    "        image_hdus[4].data = image_hdus[4].data.T[::-1, :]\n",
    "        image_hdus[4].header['DATASEC'] = '[1:2056,129:2176]'\n",
    "        image_hdus[4].header['BIASSEC'] ='[2057:2184,1:2176]'\n",
    "        image_hdus[4].header['DETSEC'] = '[1:2056, 2049:4096]'\n",
    "        image_hdus[4].header.pop('TRIMSEC')\n",
    "\n",
    "        # Copy all overlapping values into the main header\n",
    "        for i in image_hdus[1].header:\n",
    "            if i == 'COMMENT' or len(i) == 0:\n",
    "                continue\n",
    "            if image_hdus[2].header.get(i) == image_hdus[1].header.get(i):\n",
    "                image_hdus[0].header[i] = image_hdus[1].header[i]\n",
    "\n",
    "        image_hdus[0].header['TRIMSEC'] = '[1:4112,1:4096]'\n",
    "\n",
    "        for i in range(1,5):\n",
    "            image_hdus[i].header['RDNOISE'] = image_hdus[i].header['ENOISE']\n",
    "            image_hdus[i].header['GAIN']  = image_hdus[i].header['EGAIN'] \n",
    "\n",
    "        # now create an HDUList and a unique filename to store the data in\n",
    "        filename_suffix_obstype_mapping = {'Bias': 'b00',\n",
    "                                           'Flat': 'f00',\n",
    "                                           'Object': 'e00'}\n",
    "        hdu_list = fits.HDUList(image_hdus)\n",
    "        filename = f'{image_hdus[1].header[\"FILENAME\"][:-2]}-{image_hdus[1].header[\"UT-DATE\"].replace(\"-\", \"\")}-{filename_suffix_obstype_mapping[image_hdus[1].header[\"EXPTYPE\"]]}.fits.fz'\n",
    "        hdu_list.writeto(os.path.join(combined_dir, filename), overwrite=True, output_verify='silentfix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now construct the database instance for each night, and add the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context, combined_data_dir in zip(contexts, mef_combined_data_dirs):\n",
    "    if os.path.exists(os.path.join(combined_data_dir, 'test.db')):\n",
    "        os.remove(os.path.join(combined_data_dir, 'test.db'))\n",
    "    os.system(f'banzai_create_db --db-address={context.db_address}')\n",
    "    os.system(f'banzai_add_site --site LCO --latitude -29.08300 --longitude -70.698005 --elevation 2280 --timezone -4 --db-address={context.db_address}')\n",
    "    os.system(f'banzai_add_instrument --site LCO --camera Direct/4Kx4K-4 --name Direct/4Kx4K-4 --instrument-type swope-imager --db-address={context.db_address}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument = dbs.get_instruments_at_site('LCO', context.db_address)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each night, reduce and stack all bias files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context, combined_data_dir in zip(contexts, mef_combined_data_dirs):\n",
    "    bias_files = glob(os.path.join(combined_data_dir, '*b00*'))\n",
    "    for bias_file in bias_files:\n",
    "        run_pipeline_stages([{'path': bias_file}], context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_frames_as_good(filenames, context):\n",
    "    for filename in glob(f'test_data/LCO/*/*/processed/{filenames}'):\n",
    "        dbs.mark_frame(os.path.basename(filename), \"good\", db_address=context.db_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context in contexts:\n",
    "    mark_frames_as_good('*b91*', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context in contexts:\n",
    "    make_master_calibrations(instrument, 'Bias', '2022-09-20', '2022-10-03', context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each night, reduce and stack all flat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context, combined_data_dir in zip(contexts, mef_combined_data_dirs):\n",
    "    flat_files = glob(os.path.join(combined_data_dir, '*f00*'))\n",
    "    for flat_file in flat_files:\n",
    "        run_pipeline_stages([{'path': flat_file}], context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context in contexts:\n",
    "    mark_frames_as_good('*f91*', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context in contexts:\n",
    "    make_master_calibrations(instrument, 'Flat', '2022-09-20', '2022-10-03', context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now reduce the science files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context, combined_data_dir in zip(contexts, mef_combined_data_dirs):\n",
    "    science_files = glob(os.path.join(combined_data_dir, '*e00*'))\n",
    "    for science_file in science_files:\n",
    "        run_pipeline_stages([{'path': science_file}], context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
